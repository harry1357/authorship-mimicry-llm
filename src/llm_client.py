# src/llm_client.py
"""
Large Language Model Client Module

This module provides a unified interface for interacting with various Large Language Model
(LLM) APIs. It handles request formatting, response parsing, and error management for
text generation tasks in the authorship mimicry research project.

Classes:
    LLMRequest: Encapsulates generation request parameters
    LLMResponse: Standardized response structure for generated text
    BaseLLMClient: Abstract base class for LLM client implementations
    GPT51Client: OpenAI GPT-5.1 model client implementation
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional

from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables from the .env file in the project root directory
load_dotenv(Path(__file__).resolve().parents[1] / ".env")


@dataclass
class LLMRequest:
    """
    Encapsulates parameters for a text generation request to an LLM.
    
    Attributes:
        prompt_id: Unique identifier for the prompt
        author_id: Identifier for the target author being mimicked
        run_id: Experimental run identifier (corresponds to full_run parameter)
        prompt_text: The complete prompt text to be sent to the LLM
        max_tokens: Maximum number of tokens to generate in the response
        temperature: Sampling temperature for generation (controls randomness)
        seed: Random seed for reproducibility (if supported by the model)
        metadata: Additional contextual information about the request
    """
    prompt_id: str
    author_id: str
    run_id: int
    prompt_text: str
    max_tokens: int = 2000
    temperature: float = 0.8
    seed: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class LLMResponse:
    """
    Standardized structure for LLM-generated text responses.
    
    Attributes:
        prompt_id: Unique identifier matching the corresponding request
        author_id: Identifier for the target author
        run_id: Experimental run identifier
        llm_key: Identifier for the LLM model used
        generated_text: The text generated by the model
        usage: Token usage statistics and API cost information
        raw_response: Complete unprocessed API response for reference
    """
    prompt_id: str
    author_id: str
    run_id: int
    llm_key: str
    generated_text: str
    usage: Dict[str, Any]
    raw_response: Dict[str, Any]


class BaseLLMClient:
    """
    Abstract base class defining the interface for LLM client implementations.
    
    All concrete LLM client classes should inherit from this base class and
    implement the generate() method according to their specific API requirements.
    """
    def generate(self, req: LLMRequest) -> LLMResponse:
        """
        Generate text based on the provided request parameters.
        
        Args:
            req: LLMRequest object containing generation parameters
            
        Returns:
            LLMResponse object containing the generated text and metadata
            
        Raises:
            NotImplementedError: This method must be implemented by subclasses
        """
        raise NotImplementedError


class GPT51Client(BaseLLMClient):
    """
    Client implementation for OpenAI's GPT-5.1 model.
    
    This client handles communication with the OpenAI API, formats requests
    according to the API specifications, and parses responses into the
    standardized LLMResponse format.
    """
    def __init__(self, model: str = "gpt-5.1-2025-11-13") -> None:
        """
        Initialize the GPT-5.1 client with API credentials.
        
        Args:
            model: The specific GPT model version to use
            
        Raises:
            RuntimeError: If OPENAI_API_KEY environment variable is not configured
        """
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError(
                "OPENAI_API_KEY is not set. Please export it before running."
            )
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.llm_key = model

    def generate(self, req: LLMRequest) -> LLMResponse:
        """
        Generate text using the OpenAI GPT-5.1 model.
        
        Note: The OpenAI Responses API does not currently support the 'seed' 
        parameter for deterministic generation. This limitation is documented
        in the API specifications.
        
        Args:
            req: LLMRequest object containing generation parameters
            
        Returns:
            LLMResponse object with generated text and usage statistics
            
        Raises:
            Exception: If the API request fails or returns an error
        """
        response = self.client.responses.create(
            model=self.model,
            input=req.prompt_text,
            temperature=req.temperature,
            max_output_tokens=req.max_tokens,
        )

        # Extract generated text from the API response
        text = ""
        if hasattr(response, "output_text") and response.output_text:
            text = response.output_text
        else:
            try:
                text = response.output[0].content[0].text
            except Exception:
                text = ""

        usage_dict: Dict[str, Any] = {}
        if hasattr(response, "usage") and response.usage is not None:
            usage = response.usage
            usage_dict = {
                "input_tokens": getattr(usage, "input_tokens", None),
                "output_tokens": getattr(usage, "output_tokens", None),
                "total_tokens": getattr(usage, "total_tokens", None),
            }

        return LLMResponse(
            prompt_id=req.prompt_id,
            author_id=req.author_id,
            run_id=req.run_id,
            llm_key=self.llm_key,
            generated_text=text.strip(),
            usage=usage_dict,
            raw_response={},  # keep small; can expand if needed
        )


class MockLLMClient(BaseLLMClient):
    def __init__(self, llm_key: str = "mock-llm") -> None:
        self.llm_key = llm_key

    def generate(self, req: LLMRequest) -> LLMResponse:
        base = req.prompt_text[:400]
        fake_text = (
            f"[MOCK GENERATION for author {req.author_id}, run {req.run_id}]\n\n"
            f"{base}\n\n"
            "[... truncated mock output ...]"
        )
        usage = {
            "input_tokens": None,
            "output_tokens": None,
            "total_tokens": None,
        }
        return LLMResponse(
            prompt_id=req.prompt_id,
            author_id=req.author_id,
            run_id=req.run_id,
            llm_key=self.llm_key,
            generated_text=fake_text,
            usage=usage,
            raw_response={"mock": True},
        )


def get_llm_client(llm_key: str) -> BaseLLMClient:
    key = llm_key.lower()
    if key in {"gpt-5.1", "gpt5.1", "gpt5_1", "gpt-5", "gpt-5.1-2025-11-13"}:
        return GPT51Client(model="gpt-5.1-2025-11-13")
    if key in {"mock", "fake"}:
        return MockLLMClient()
    raise ValueError(f"Unknown llm_key: {llm_key}")